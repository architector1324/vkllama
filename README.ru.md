# Vulkan LLaMA

**vkllama** - это лёгкий и быстрый HTTP-сервер для запуска LLaMA-подобных моделей локально с использованием **Vulkan-бэкенда**, основанный на [llama-cpp-python](https://github.com/abetlen/llama-cpp-python).

Проект предоставляет **совместимый с Ollama API**, так что вы можете использовать существующие клиенты и интерфейсы без изменений.
