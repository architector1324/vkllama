# Vulkan LLaMA

**vkllama** — это легковесный и быстрый HTTP-сервер для локального запуска моделей на базе LLaMA с использованием **бэкенда Vulkan**, построенный на основе [llama-cpp-python](https://github.com/abetlen/llama-cpp-python).

Проект предоставляет **API, совместимый с Ollama**, что позволяет использовать существующие клиенты и инструменты, предназначенные для Ollama, без каких-либо изменений.

---

## Возможности

*   **Один бинарный файл, без сторонних зависимостей в процессе выполнения**: После сборки `vkllama` представляет собой самодостаточный исполняемый файл, не требующий внешних зависимостей во время выполнения, что делает его очень портативным.
*   **Бэкенд Vulkan**: Использует `llama-cpp-python` с поддержкой Vulkan для эффективного инференса LLM на совместимых GPU.
*   **API, совместимый с Ollama**: Предоставляет эндпоинты `/api/generate`, `/api/chat` и `/api/tags`, позволяя интегрироваться с существующими клиентами и инструментами Ollama.
*   **Легковесный HTTP-сервер**: Построен с использованием стандартного модуля Python `http.server`, обеспечивая минимальные накладные расходы.
*   **Полнофункциональный CLI**: Удобный интерфейс командной строки для запуска моделей, просмотра списка доступных моделей и запуска сервера.
*   **Простая настройка**: Включает скрипт `build.sh` для настройки окружения и создания исполняемого файла, а также файл службы Systemd для развертывания в рабочей среде.

## Предварительные требования

Перед тем как начать сборку `vkllama` из исходного кода, убедитесь, что в вашей системе установлено следующее:

*   **Git**: Для клонирования репозитория.
*   **Python 3.x**: (рекомендуется 3.10+)
*   **CMake**: Требуется для сборки `llama-cpp-python`.
*   **Vulkan-совместимый GPU и драйверы**: Убедитесь, что в вашей системе установлены актуальные драйверы Vulkan для вашего GPU.

## Установка

У вас есть два основных варианта для запуска `vkllama`: загрузить предварительно собранный бинарник или собрать его из исходного кода.

### Загрузка готового исполняемого файла (Рекомендуется)

Предварительно собранные бинарники `vkllama` доступны в [разделе релизов этого репозитория](https://github.com/architector1324/vkllama/releases).

1.  **Загрузите последний релиз**:
    Загрузите исполняемый файл `vkllama` для вашей операционной системы.

2.  **Сделайте `vkllama` исполняемым и глобально доступным (Необязательно, но рекомендуется)**:

    ```bash
    chmod +x vkllama # Сделать исполняемым
    sudo mv vkllama /usr/local/bin/ # Переместить в директорию, находящуюся в PATH
    ```

    Теперь вы сможете запускать `vkllama` из любого каталога в вашем терминале.

### Сборка из исходного кода

Этот вариант требует наличия инструментов разработки, таких как Python, Git и CMake.

1.  **Клонируйте репозиторий**:

    ```bash
    git clone https://github.com/yourusername/vkllama.git # Замените на фактический URL вашего репозитория, если он отличается
    cd vkllama
    ```

2.  **Соберите исполняемый файл**:

    Скрипт `build.sh` настраивает виртуальное окружение Python, устанавливает необходимые зависимости (включая `llama-cpp-python` с поддержкой Vulkan) и упаковывает приложение в один исполняемый файл с помощью PyInstaller.

    ```bash
    ./build.sh
    ```

    После успешной сборки исполняемый файл `vkllama` будет создан в корневой директории вашего проекта.

3.  **Сделайте `vkllama` глобально доступным (Необязательно, но рекомендуется)**:

    Переместите исполняемый файл `vkllama` в директорию, находящуюся в PATH вашей системы, например, `/usr/local/bin/`:

    ```bash
    sudo mv vkllama /usr/local/bin/
    ```

    Теперь вы сможете запускать `vkllama` из любого каталога в вашем терминале.

## Настройка моделей

`vkllama` использует модели в формате GGUF. Вам необходимо загрузить нужные модели и настроить их для сервера.

1.  **Создайте директорию для моделей**:

    По умолчанию `vkllama` ожидает, что ваши GGUF-модели и файл конфигурации `models.json` будут находиться в `~/.vkllama/models/`. Создайте эту директорию, если она не существует:

    ```bash
    mkdir -p ~/.vkllama/models/
    ```

2.  **Загрузите GGUF-модели**:

    Вы можете загрузить GGUF-модели из различных источников, например, с [Hugging Face](https://huggingface.co/models?search=gguf). Разместите загруженные файлы `.gguf` в директории `~/.vkllama/models/`.

    Например, если вы загрузите `gemma-3-4b-it-Q4_K_M.gguf`, поместите его в `~/.vkllama/models/`.

3.  **Создайте `models.json`**:

    В той же директории `~/.vkllama/models/` создайте файл с именем `models.json`. Этот файл сообщает `vkllama`, какие модели он может обслуживать.

    Вот пример `models.json`:

    ```json
    [
        {
            "name": "gemma3",
            "filename": "gemma-3-4b-it-Q4_K_M.gguf",
            "digest": "be49949e48422e4547b00af14179a193d3777eea7fbbd7d6e1b0861304628a01",
            "quantization_level": "Q4_K_M",
            "parameter_size": "4.3B"
        },
        {
            "name": "qwen3",
            "filename": "Qwen3-8B-Q4_K_M.gguf",
            "digest": "d98cdcbd03e17ce47681435b5150e34c1417f50b5c0019dd560e4882c5745785",
            "quantization_level": "Q4_K_M",
            "parameter_size": "8.2B"
        },
        {
            "name": "qwen3:4b",
            "filename": "Qwen3-4B-Q4_K_M.gguf",
            "digest": "7485fe6f11af29433bc51cab58009521f205840f5b4ae3a32fa7f92e8534fdf5",
            "quantization_level": "Q4_K_M",
            "parameter_size": "4.0B"
        },
        {
            "name": "gemma3n",
            "filename":"gemma-3n-E4B-it-Q4_K_M.gguf",
            "digest": "7fcb647151fa19a0750538672cf824ef6cf18f74bb86ebe5592e1ed59b4070a0",
            "quantization_level": "Q4_K_M",
            "parameter_size": "6.9B"
        }
    ]
    ```

    **Описание полей для `models.json`**:
    *   `name`: Уникальный идентификатор вашей модели, используемый при запуске или просмотре списка моделей (например, `vkllama run -m gemma3`).
    *   `filename`: Точное имя файла GGUF-модели в вашей директории моделей.
    *   `digest`: (Необязательно, но рекомендуется) SHA256-хэш файла модели. Если указан, `vkllama` будет использовать это значение; в противном случае он рассчитает его на лету (что может занять время для больших файлов).
    *   `quantization_level`: Описывает уровень квантования модели (например, `Q4_K_M`).
    *   `parameter_size`: Указывает количество параметров модели (например, `4.3B`).

## Запуск сервера

Вы можете запустить сервер `vkllama` вручную или настроить его как службу Systemd для автоматического управления.

### Ручной запуск сервера

Для ручного запуска сервера:

```bash
vkllama serve [--host 0.0.0.0] [--port 11434] [--models ~/.vkllama/models]
```

*   `--host`: IP-адрес, к которому будет привязан сервер. (По умолчанию: `0.0.0.0`)
*   `--port`: Порт, на котором будет прослушивать сервер. (По умолчанию: `11434`)
*   `--models`: Путь к вашей директории моделей, содержащей `models.json` и GGUF-файлы. (По умолчанию: `~/.vkllama/models`)

Пример:
```bash
vkllama serve
```
Вы увидите вывод, указывающий на запуск сервера, например: `Starting vkllama server on http://0.0.0.0:11434`

### Служба Systemd (Рекомендуется для продакшена)

Использование Systemd гарантирует, что `vkllama` будет автоматически запускаться при загрузке, перезапускаться в случае сбоя и логировать свой вывод в системный журнал.

1.  **Скопируйте файл службы**:

    ```bash
    sudo cp src/vkllama.service /etc/systemd/system/
    ```

2.  **Отредактируйте файл службы**:

    Вы **обязательно** должны отредактировать файл `vkllama.service`, чтобы заменить заполнитель `User=arch` на ваше фактическое имя пользователя.

    ```bash
    sudo nano /etc/systemd/system/vkllama.service
    # ИЛИ (замените 'your_username' на ваше фактическое имя пользователя)
    sudo sed -i "s/User=arch/User=$(whoami)/" /etc/systemd/system/vkllama.service
    ```

3.  **Перезагрузите Systemd, включите и запустите службу**:

    ```bash
    sudo systemctl daemon-reload       # Перезагружает Systemd для распознавания новой службы
    sudo systemctl enable --now vkllama.service # Включает службу для запуска при загрузке и запускает ее сейчас
    ```

4.  **Проверьте статус службы и логи**:

    ```bash
    systemctl status vkllama.service
    journalctl -u vkllama.service -f # Просмотр логов в реальном времени
    ```

## Использование интерфейса командной строки (CLI)

`vkllama` предоставляет простой CLI для взаимодействия с запущенным сервером.

### Просмотр доступных моделей

Чтобы просмотреть список моделей, настроенных на сервере `vkllama`:

```bash
vkllama list [--address 0.0.0.0:11434]
```

Пример вывода:

```
NAME         ID           SIZE      MODIFIED
gemma3       be49949e4842 4.3 GB    3 days ago
qwen3        d98cdcbd03e1 8.2 GB    2 weeks ago
qwen3:4b     7485fe6f11af 4.0 GB    1 month ago
gemma3n      7fcb647151fa 6.9 GB    5 days ago
```

### Запуск LLM-модели

Для генерации ответа от модели:

```bash
vkllama run -m <model_name> [ОПЦИИ] <промпт>
```

*   `-m` / `--model`: Имя модели для использования (как определено в `models.json`).
*   `--seed`: Укажите числовое начальное значение для воспроизводимой генерации текста.
*   `-s` / `--stream`: Включите потоковый вывод (ответ появляется по словам).
*   `-t` / `--think`: Включите расширенное, итеративное рассуждение. *Примечание: Этот флаг в настоящее время не реализован в логике сервера.*
*   `-a` / `--address`: Адрес хоста сервера (например, `localhost:11434`). (По умолчанию: `0.0.0.0:11434`)
*   `prompt`: Текстовый промпт для модели. Заключите в кавычки, если содержит пробелы.

Пример:

```bash
vkllama run -m gemma3 "What is the capital of France?"
```

Пример с потоковым выводом:

```bash
vkllama run -m qwen3 -s "Explain quantum computing in simple terms."
```

## Совместимость с Ollama

Поскольку `vkllama` реализует API, совместимый с Ollama, вы можете использовать любой клиент, библиотеку или приложение, разработанное для работы с Ollama. Просто настройте ваш клиент Ollama, чтобы он указывал на адрес и порт вашего сервера `vkllama` (например, `http://localhost:11434`).

Это позволяет вам использовать бэкенд `vkllama` с его преимуществами производительности Vulkan, одновременно пользуясь привычной экосистемой Ollama.
