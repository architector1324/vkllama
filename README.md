# Vulkan LLaMA

**vkllama** is a lightweight and fast HTTP server for running LLaMA-based models locally using the **Vulkan backend**, built on top of [llama-cpp-python](https://github.com/abetlen/llama-cpp-python).

The project provides an **Ollama-compatible API**, so you can use existing clients and tools without any changes.
